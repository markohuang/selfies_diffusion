{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile, json, sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path().resolve().parent))\n",
    "from selfies_diffusion.selfies_grammar import grammar\n",
    "from base_tokenizer import output_json\n",
    "from tokenizers import Tokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers.tokenization_utils import AddedToken\n",
    "\n",
    "all_tokens = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar.all_tokens()\n",
    "slist = [\"B\", \"=B\", \"#B\", \"/B\", r\"\\B\",\n",
    "    \"C\", \"=C\", \"#C\", \"/C\", r\"\\C\",\n",
    "    \"N\", \"=N\", \"#N\", \"/N\", r\"\\N\",\n",
    "    \"O\", \"=O\", \"#O\", \"/O\", r\"\\O\",\n",
    "    \"S\", \"=S\", \"#S\", \"/S\", r\"\\S\",\n",
    "    \"P\", \"=P\", \"#P\", \"/P\", r\"\\P\",\n",
    "    \"F\", \"=F\", \"#F\", \"/F\", r\"\\F\",\n",
    "    \"Cl\", \"=Cl\", \"#Cl\", \"/Cl\", r\"\\Cl\",\n",
    "    \"Br\", \"=Br\", \"#Br\", \"/Br\", r\"\\Br\",\n",
    "    \"I\", \"=I\", \"#I\", \"/I\", r\"\\I\",\n",
    "    \"Ring1\", \"=Ring1\", \"#Ring1\", \"/Ring1\", r\"\\Ring1\",\n",
    "    \"Ring2\", \"=Ring2\", \"#Ring2\", \"/Ring2\", r\"\\Ring2\",\n",
    "    \"Ring3\", \"=Ring3\", \"#Ring3\", \"/Ring3\", r\"\\Ring3\",\n",
    "    \"Branch\", \"=Branch\", \"#Branch\",\n",
    "    \"->\", \"pop\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../chembl_tokenizer/tokenizer_config.json',\n",
       " '../chembl_tokenizer/special_tokens_map.json',\n",
       " '../chembl_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens.add('[:0furan]')\n",
    "all_tokens = all_tokens.union(\n",
    "    grammar.all_tokens(),\n",
    "    set(f'[{s}]' for s in slist)\n",
    ")\n",
    "\n",
    "start_idx = 7\n",
    "vocab = output_json['model']['vocab']\n",
    "vocab.update({\n",
    "  e: idx for idx, e in enumerate(list(all_tokens), start_idx)  \n",
    "})\n",
    "\n",
    "with tempfile.NamedTemporaryFile(delete=True) as tmp_file:\n",
    "    with open(tmp_file.name, 'w') as f:\n",
    "        json.dump(output_json, f, indent=2)\n",
    "    tokenizer = PreTrainedTokenizerFast(\n",
    "        tokenizer_object=Tokenizer.from_file(tmp_file.name)\n",
    "    )\n",
    "bos_token = AddedToken('<bos>', lstrip=False, rstrip=False)\n",
    "eos_token = AddedToken('<eos>', lstrip=False, rstrip=False)\n",
    "sep_token = AddedToken('<sep>', lstrip=False, rstrip=False)\n",
    "cls_token = AddedToken('<cls>', lstrip=False, rstrip=False)\n",
    "unk_token = AddedToken('<unk>', lstrip=False, rstrip=False)\n",
    "pad_token = AddedToken('<pad>', lstrip=False, rstrip=False)\n",
    "mask_token = AddedToken('<mask>', lstrip=True, rstrip=False) # include space in front\n",
    "num_tokens_added = tokenizer.add_special_tokens({\n",
    "    'bos_token': bos_token,\n",
    "    'eos_token': eos_token,\n",
    "    'sep_token': sep_token,\n",
    "    'unk_token': unk_token,\n",
    "    'cls_token': cls_token,\n",
    "    'pad_token': pad_token,\n",
    "    'mask_token': mask_token,\n",
    "})\n",
    "assert num_tokens_added == 0\n",
    "tokenizer.model_max_length = 9999 # this shouldn't matter\n",
    "tokenizer.save_pretrained('../chembl_tokenizer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "tset, vset = Dataset.from_text('../chembl_selfies_subset.txt').train_test_split(0.2).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cc1c(CCC(=O)NCCC(=O)O)c(=O)oc2cc3occ(-c4ccccc4)c3cc12'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from group_selfies import GroupGrammar\n",
    "from rdkit.Chem import MolFromSmiles, MolToSmiles\n",
    "MolToSmiles(grammar.decoder(tset[121]['text'].replace(' ', '')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "selfies_diff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
